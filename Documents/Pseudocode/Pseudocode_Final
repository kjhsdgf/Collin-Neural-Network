#include<iostream>
#include<fstream>
#include<string.h>
#include<vector.h>
#include<dlib/matrixabstract.h>
	
using namespace std;
using namespace dlib;

void main()
{
	//Display the menu to ask whether the user wants to train or classify the network 
	//If user chooses to train a new network:
		Network N;				
		//readIn() will be called inside the default constructor
		//remember to check the validity of the parameters user entered!

		//call the train() method in the class Network		
		N.train();
	
	//If the user chooses to continue training an existing network:
		//pass the constructor a string that matches the previous network's filename
		Network N(previous_network_filename);
		
		//This will ask if the user would like to change any hyperparameters
		//Then proceed to train as above
		N.train();
			
	//If user chooses to Classify:
		//ask the user to enter strings that represent the previous network filename and validation filename
		Network N(previous_network_filename, validation_file_name);	

		//or call the classify() method in the class Network
		//after the member is initialized
		N.classify();
}

#ifdef _DEBUG
	//TBD
#endif

#include <iostream>
#include <fstream>
#include <time.h>
#include <string>
#include <vector.h>
#include <dlib...>
	
using namespace std;
using namespace dlib;

Class Network
{
	Data Members:			//Names of the Data Members can be changed later
		double eta;
		int batch_size;
		int epochs;
		int layer_sizes[];          //array of layer sizes
		int L;                      //number of layers i.e. sizeof(layer_sizes)
		vector<matrix> weights;
		vector<matrix> biases;

		vector<matrix> activations;
		vector<matrix> weighted_inputs;

		vector<matrix> delta; 
		vector<matrix> sum_nabla_b;
		vector<matrix> sum_nabla_w;

		vector<int> test data indices;
		vector<int> mini batch indices;		//only used in train(), why not limit its scope to that function?

		int number correct;			//only used in train(), why not limit its scope to that function?

		string training_data_filename;		//consider truncating "filename". unnecessary and unwieldy
		istream training_data_infile
		string expected_values_filename;	//	adding "filename" also makes code easier to read and understand
		istream expected_values_infile		//	when you have other related data structures to specify (-Eli)
		
	Methods:
	
	Network()
        {
			readIn();
			//open training_data_file with istream object
			//open expected_values_file with istream object

			//Generate L, which is the number of layers. This is given by sizeof(layer sizes). 
				//Weights, biases, activations, weighted inputs, nabla_w, nabla_b resized to size L.
				//Everything but the activations vector will have an effective size of L-1, as their first element will be left unused.
				//Looping an index i from 1 to L
					//resize the ith element of the weights vector: layer sizes at i by layer sizes at i-1, fill w/ rand #s using randomizeMatrix()
					//resize the ith element of the sum_nabla_w vector: layer sizes at i by layer sizes at i-1, fill with zeros (yes, fill with zeros -Eli)	
					//resize the ith element of the biases vector: layer sizes at i by 1, fill with random numbers using randomizeMatrix()
					//resize the ith element of the sum_nabla_b vector: layer sizes at i by 1, fill with zeros
					//resize the ith element of the delta vector: layer sizes at i by 1, fill with zeros				
					//resize the ith element of the activations vector: layer sizes at i by 1, fill with zeros
					//resize the ith element of the weightedinputs vector: layer sizes at i by 1, fill with zeros
			//end of the loop

			//resize the 0th member of the activations vector: layer sizes at 0 by 1, fill with zeros			
			//resize mini batch indices to batch_size			            		
        }
				
	//consider overloading the constructor for classification
	//	heck yes! that's a great idea! (-Eli)
	Network(const string &previous_network_filename, const string& validation_data_filename)
	{
		//training_data_filename = validation_data_filename; <-- though efficient, this may confuse people reading code down the line
									//maybe we could generalize the name of training_data or
									//just make another data member for validation data
		//open previous_network_filename
		//open validation_data_filename
		//readInit(previous_network_filename)
		//classify(validation_data_filename)
	}
	
	Network(const string &previous_network_filename)
	{
	    //readIn(previous_network_filename);
		//open training_data_file with istream object
	    //open expected_values_file with istream object		
		//prompt user: do they want to update hyperparameters before training?
			//if so, update via cin
	}

	~Network()
	{
		//save network data to a file using createNetworkFile()
		//make sure to close all the files, namely training_data and expected_values_file
		//deallocate dynamic memory
	}

	//no need to return filename if it's already written to a data member (-Eli)
	//in case user wants to load a previous network to continue training, or update hyperparameters and retrain
	void readInit(const string &previous_network_filename)
    {
	 	//Read in the hyperparameters from file and store them in the data members
		//Read in the name of expected_values_file, training_data_filename
	    //Read in L, which is the number of layers.
		//Weights, biases, activations, weighted inputs, nabla_w, nabla_b resized to size L.
        	//Everything but the activations vector will have an effective size of L-1, as their first element will be left unused.
        //Looping an index i from 1 to L
			//resize the ith element of the weights vector: layer sizes at i by layer sizes at i-1, fill with zeros
			//resize the ith element of the sum_nabla_w vector: layer sizes at i by layer sizes at i-1, fill with zeros (yes, fill with zeros -Eli)	
			//resize the ith element of the biases vector: layer sizes at i by 1, fill with zeros
			//resize the ith element of the sum_nabla_b vector: layer sizes at i by 1, fill with zeros
			//resize the ith element of the delta vector: layer sizes at i by 1, fill with zeros				
			//resize the ith element of the activations vector: layer sizes at i by 1, fill with zeros
			//resize the ith element of the weightedinputs vector: layer sizes at i by 1, fill with zeros
		//end of the loop
		//Assign the matrices created with the weights and biases from previous_network_filename
		//resize the 0th member of the activations vector: layer sizes at 0 by 1, fill with zeros			
	    //resize mini batch indices to batch_size
	}
	 
	void readInit()
    {
		//Prompt the user to enter details through the command prompt, storing them in the data members
    }
	
	//takes a matrix object and the address to a function (returns a double, no arguments)
	//assigns numbers as specified by the function passed
	void randomizeMatrix(matrix<double> input_matrix, double (*distribution)(void))
	{
		//Looping an index j from 1 to input_matrix.nr()
			//Looping an index i from 1 to input_matrix.nc()
				//input_matrix(i, j) = distribution();
	}

	//forwardPropagation() is a function that sets all activation values for a single test data input. 
	//It needs the layer of activations at 0 to be assigned values from the test data.
	void forwardPropagation(matrix<double> input_vector)
	{
		//activations at 0 = input_vector
		//Looping an index i from 1 to L
			//weighted inputs at i = (weights at i * activations at i-1) + biases at i;
			//activations at i = activation_Function(weighted_inputs at i, Layer_Size at i);
	}
		
	//backPropagation() is a function that calculates the nabla_b and nabla_w vectors.
	//backProp requires activationPrime function, costPrime function, activations and weighted inputs already been set
	void backPropagation(matrix of expected output for a given training input, expected_val)
		{
			//compare expected val to activations[L] (using similar algorithm as seen in classify())
			//if they are the same, increment number correct 

			//delta at L = hadamardProduct(costPrime(Activations at L, expected_val), sigmoidPrime(weighted inputs at L))
			//sum_nabla_b at L += delta at L
			//sum_nabla_w at L += (delta at L) * (activations at L-1 transposed)
			//Looping an index i from L-1 to 1 (the remaining layers)
				//delta at i = hadamardProduct(((weights at i + 1 transposed)*(delta at i + 1)), sigmoidPrime(weighted inputs of at i)
				//sum_nabla_b at i += delta at i 
				//sum_nabla_w at i += (delta at i) * (activations at i-1 transposed)
			//end of the loop
		}
		
	//SGD's function is to complete a forward pass and backward pass on a mini batch and compute the average nabla_b and 
	//nabla_w vectors over the whole batch. it then updates the weights and biases using the nabla vectors and the learning
	//rate eta
	//SGD requires an int batch size to iterate over the vector mini batch indices which hold indices of the test data
	//  with respect to the mini batch to be iterated
	//  it is assumed that these indices are randomly generated and won't repeat a test element for the same epoch but that
	//  is outside SGD's scope and it won't need to worry about that.
	//SGD also requires test data indices to have been resized appropriately
	
	//may not be the same as mini batch size due to leftover data at end of test data
	//	We're throwing out the leftovers as per discussion in the meeting on 9-22-17 (-Eli)
	void SGD(int batch_size)
	{
		//compute average errors for the batch
		//create a vector for expected values, expected_val
		//create a vector for input into the network, input_activations
		//Looping index i from 0 to batch_size
			//read next input_activations
			//forwardPropagation(input_activations)
			//read next expected_val
			//backwardPropagation(expected_val)

		//apply error information to weights and biases
		//update();                
		//zero out sum_nabla vectors
	}
		
	void update()
	{
		//Looping index i from 1 to L
			//weights[i] -= (eta / batch_size) * (sum_nabla_w[i]) <-- I don't think there's a way to get around doing one division
			//biases[i]  -= (eta / batch_size) * (sum_nabla_b[i])		per element. You either do it in the loop or not.
	}
		
	//Train brings all the other methods together and trains the network. It loops through the epochs separating out 
	//mini batches from the test data and running sgd on all mini batches for each epoch. Train doesnt actually access the files
	void train()
	{
		//jump to the beginning of the file
		//open the truth data file 
		int test data size = size of test data
		test data indices resized to test data size and initialized:
			such that test data indices at i = i    // these values but will be randomized later

		Loop index i through test data indices
			test data indices[i] = i

		int sgd calls = (size of test data)/(mini batch size)  // number of times sgd is called to finish one epoch 
		int leftover = epochs remainder(%) mini batch size     // to see if there's a small mini batch left in the data
								       //we can use leftover later to determine the consequences of not performing SGD(leftover)
		Looping index i from 0 to epochs
			number_correct = 0;
			Fisher Yates shuffle (test data indices)
			int batch size = mini batch size

			Looping index j from 0 to sgd calls
				Looping through index k from 0 to (mini batch size - 1)
					mini batch indices[k] = test data indices [(j*mini batch size) + k)]
				SGD(batch size)
			
			cout << "Efficiency at epoch: " << i << " = " << ((number correct) / (test data size))*100 <<" %";
		//end of the loop
		
		bool result = Create_Network_File(); 		//this will store the values required for classification
		if(!result)
			//Display an error message
	}
	
	bool createNetworkFile()
	{
		//create a file named Previous_Network_[Date and Time] to write the required values 
		if (is_open(Previous_Network_[Date and Time])) 
			//write the values of hyperparameters, L, array of layer sizes, matrices of weights and biases
			//close the file
			return true;
		else
			return false;
	}
		
	void classify() 				//we don't need any parameters here, because constructor stores the name of validation file in the training_data_file
	{
		//int ambiguous_data (0);
		//int numTrainingInputs;		//will tell us the number of the training inputs in the verfification file
		//int biggest;
		//to calculate numTrainingInputs, we can call tellg(), which will give us the size of file in bytes so that 
		//numTrainingInputs = size of file/size of a single verification datum
		//Loop index i from 0 to size of (verification data) //from i = 0 to end
			//call forwardPass(), which takes a datum from training_data_filename and computes the activations <-- is this the right file?
			//set biggest = 0, which will act as the index of the biggest activation value
			//Looping index j from 1 to size of (activations[L]) (j=0 is assumed to be biggest)
				//if activations(L, j) > activations(L, biggest) 
					//then biggest = j
				//else if activations(L, j) == activations(L, biggest)
					//increment ambiguous_data;
				//else
					//do nothing.
			//print out classification of biggest
		
		//print out the number of data classified, which will be (size of verification data) - ambiguous_data
			// i.e. what output the network has decided, for our purposes a string  "horizontal" "vertical" or "diagonal"
	}
		
	//hadamardProduct() is a operation between two matrices where we obtain a matrix by multiplying an element in one matrix 
    //with that respective element in the other matrix.
	//hadamardProduct requires that M1 and M1 are of equal size
   	matrix& hadamardProduct (const matrix& M1, const matrix& M2)
	{
	  //matrix M;
	  //check the size of the matrices
	  //if equal:
		  //Create a matrix M of same size as that of either M1 or M2
		  //Start the loop for i = 1 to i = the number of columns in M1 
		      //Start an inner loop for j = 1 to j = number of rows in M1
			  	//Every element in M, M(j, i) = M1(j, i) * M2(j, i); <-- how dlib handles single elements, iirc
		      //end of the loop
		  //end of the loop
		  //return the matrix M

	      //else
		  //throw an exception or display the error message, return an empty matrix;
	}
	  
	//shuffleDataIndices() will be called at the starting of every epoch to randomize the data provided by the user
	//This will create a vector<int> of the same size as that of Data Set and every element of the vector will point to 
	//a particular input data in the data file.
	//shuffleDataIndices() will randomize this array of int by swapping different indices, using Fisher-Yates shuffle
	//This will assign created vector<int>, once done with the shuffling
	void shuffleDataIndices(vector<T> data_indices)
	{
	  //Loop index i from 0 to (size of expected_values data set - mini_batch - 1)
		//j = random integer in the range [mini_batch + i, size of expected_values data set]
		//exchange data_indices[i] with data_indices[j];
	}
};

//Functions outside of the class will include:
matrix& activationFunction(const matrix& weighted_inputs, const int Layer_Size)
{
	//return sigmoid(weighted_inputs)
}

matrix& costPrime(const matrix& input_activation_layer, const matrix& expected_val)
{
	//creating an entire matrix every time you need to evaluate cost_prime will extend the time to process data
	
	//return (input_activation_layer - expected_val)
}

matrix& activationPrime(const matrix& input_matrix)
{
	//we don't need to do this element-by-element because dlib's sigmoid function can handle an
	//entire matrix as input, we need the Hadamard product though
	
	//Make a matrix of ones, named ones, size the same as input_matrix
	//return hadamardProduct(sigmoid(input_matrix), ones - sigmoid(input_matrix))
}

//doubtful about including Randomize_Weights() outside the class
//	The user may want to mess with the distribution, 
//	but we can write it into the class for now if it helps troubleshooting (-Eli)
