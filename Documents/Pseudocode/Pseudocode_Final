#include "Network.h"

void main()
{
	//Display the menu to ask whether the user wants to train or classify the network 
	//If user chooses to train a new network:
		Network N;				
		//readIn() will be called inside the default constructor
		//remember to check the validity of the parameters user entered!

		//call the train() method in the class Network		
		N.train();
	
	//If the user chooses to continue training an existing network:
		//pass the constructor a string that matches the previous network's filename
		Network N(previous_network_filename);
		
		//This will ask if the user would like to change any hyperparameters
		//Then proceed to train as above
		N.train();
			
	//If user chooses to Classify:
		//ask the user to enter strings that represent the previous network filename and validation filename
		Network N(previous_network_filename, validation_file_name);	

		//or call the classify() method in the class Network
		//after the member is initialized
		N.classify();
}

#ifdef _DEBUG
	//TBD
#endif

#include <iostream>
#include <fstream>
#include <time.h>
#include <string>
#include <vector.h>
#include <dlib...>
	
using namespace std;
using namespace dlib;

Class Network
{
	Data Members:			//Names of the Data Members can be changed later
		double learning_rate;
		int batch_size;
		int epochs;
		vector<int> layer_sizes; //because we don't know what size will it be
		int num_layers;
		vector<matrix> weights;
		vector<matrix> biases;
		vector<matrix> activations;
		vector<matrix> weighted_inputs;
		vector<matrix> errors; 
		vector<matrix> sum_nabla_b;
		vector<matrix> sum_nabla_w;
		vector<int> test_data_indices;
		vector<int> mini_batch_indices;		//only used in train(), why not limit its scope to that function? //I agree [-Ami]
		int num_correct;			//only used in train(), why not limit its scope to that function? //Yes [-Ami]
		string training_data;		//consider truncating "filename". unnecessary and unwieldy
		ifstream training_data_infile
		string expected_values_filename;	//	adding "filename" also makes code easier to read and understand
		ifstream expected_values_infile		//	when you have other related data structures to specify (-Eli)
							//	ifstream will work specifically for the files, have no idea for the other data structures [-Ami]	
	Methods:
	
	Network()
        {
			readIn();
			//open training_data_filename with ifstream object
			//open expected_values_filename with ifstream object

			//Generate num_layers, which is the number of layers. This is given by sizeof(layer sizes). 
			//weights, biases, activations, weighted inputs, nabla_w, nabla_b resized to size num_layers.
				//Everything but the activations vector will have an effective size of num_layers-1, as their first element will be left unused.
			//expected_values vector resized to match number of 
			//Looping an index i from 1 to num_layers
					//resize the ith element of the weights vector: layer_sizes[i] by layer_sizes[i-1], fill with zeros
					//resize the ith element of the sum_nabla_w vector: layer_sizes[i] by layer_sizes[i-1], fill with zeros <-- (yes, fill with zeros -Eli)	
					//resize the ith element of the biases vector: layer_sizes[i] by 1, fill with zeros
					//resize the ith element of the sum_nabla_b vector: layer_sizes[i] by 1, fill with zeros
					//resize the ith element of the errors vector: layer_sizes[i] by 1, fill with zeros				
					//resize the ith element of the activations vector: layer_sizes[i] by 1, fill with zeros
					//resize the ith element of the weighted_inputs vector: layer_sizes[i] by 1, fill with zeros
					//resize the ith element of the expected_values vector: layer_sizes[L] by 1, fill with values from file
			//end of the loop

			//resize the 0th member of the activations vector: layer_sizes[0] by 1, fill with zeros			
			//resize mini_batch_indices to batch_size			            		
        }
				
	//consider overloading the constructor for classification
	//	heck yes! that's a great idea! (-Eli)
	Network(const string &previous_network_filename, const string& validation_data_filename)
	{
		//training_data_filename = validation_data_filename; <-- though efficient, this may confuse people reading code down the line
									//maybe we could generalize the name of training_data or
									//just make another data member for validation data
		//open previous_network_filename
		//open validation_data_filename
		//readInit(previous_network_filename)
		//classify(validation_data_filename)
	}
	
	Network(const string &previous_network_filename)
	{
	    //readIn(previous_network_filename);	
		//prompt user: do they want to update hyperparameters before training? //	why not, read the hyperparameters form the file only if the user wants it? [-Ami]
			//if so, update via cin
	}

	~Network()
	{
		//save network data to a file using createNetworkFile()
		//make sure to close all the files, namely training_data_infile and expected_values_infile
		//deallocate dynamic memory
	}

	//no need to return filename if it's already written to a data member (-Eli) //Agreed! [-Ami]
	//in case user wants to load a previous network to continue training, or update hyperparameters and retrain
	void readInit(const string &previous_network_filename)
    {
	 	//Read in the hyperparameters from previous_network_filename and store them in the data members
		//Read in the name of expected_values_filename, training_data_filename
		//open training_data_filename with ifstream object
	    //open expected_values_filename with ifstream object			
	    //Read in num_layers, which is the number of layers.
		//Weights, biases, activations, weighted inputs, nabla_w, nabla_b resized to size num_layers.
        	//Everything but the activations vector will have an effective size of num_layers-1, as their first element will be left unused.
        //Looping an index i from 1 to num_layers
			//resize the ith element of the weights vector: layer_sizes[i] by layer_sizes[i-1], fill with zeros
			//resize the ith element of the sum_nabla_w vector: layer_sizes[i] by layer_sizes[i-1], fill with zeros
			//resize the ith element of the biases vector: layer_sizes[i] by 1, fill with zeros
			//resize the ith element of the sum_nabla_b vector: layer_sizes[i] by 1, fill with zeros
			//resize the ith element of the errors vector: layer_sizes[i] by 1, fill with zeros				
			//resize the ith element of the activations vector: layer_sizes[i] by 1, fill with zeros
			//resize the ith element of the weighted_inputs vector: layer_sizes[i] by 1, fill with zeros
			//resize the ith element of the expected_values vector: layer_sizes[L] by 1, fill with values from file
		//end of the loop
		//Assign the matrices created with the weights and biases from previous_network_filename
		//resize the 0th member of the activations vector: layer_sizes[0] by 1, fill with zeros
	    //resize mini_batch_indices to batch_size
	}
	 
	void readInit()
    {
		//Prompt the user to enter details through the command prompt, storing them in the data members
    }
	
	//takes a matrix object and the address to a function (returns a double, no arguments)
	//assigns numbers as specified by the function passed
	void randomizeMatrix(matrix<double> input_matrix, double (*distribution)(void)) //I am still thinking about distribution(), need to look up for that [-Ami]
	{
		//Looping an index j from 1 to input_matrix.nr()
			//Looping an index i from 1 to input_matrix.nc()
				//input_matrix(i, j) = distribution(); 
	}

	//forwardPropagation() is a function that sets all activation values for a single test data input. 
	//It needs the layer of activations[0] to be assigned values from the test data.
	void forwardPropagation(int mini_batch_index)
	{
		//activations[0] = (contents in file[mini_batch_index]) <-- can either do this elementwise once, or elementwise (to load an input vector), then using dlib's overriden = operator
		//Looping an index i from 1 to num_layers			// I would say to read from the file, use strtok to assign the values in the activations matrix and this has to be done element wise if we're using strtok. [-Ami]
			//weighted inputs[i] = (weights[i] * activations[i-1]) + biases[i];
			//activations[i] = activation_Function(weighted_inputs[i]);
	}
		
	//backPropagation() is a function that calculates the nabla_b and nabla_w vectors.
	//backProp requires activationPrime function, costPrime function, activations and weighted inputs already been set
	void backPropagation(int mini_batch_index) <-- give the function an integer corresponding to the sample you want to backprop	//yes, this works! [-Ami]
		{
			//compare contents in expected_values[mini_batch_index] to activations[num_layers] (using similar algorithm as seen in classify())
			//if they are the same, increment num_correct 

			//errors[num_layers] = hadamardProduct(costPrime(activations[num_layers], expected_values[mini_batch_index]), sigmoidPrime(weighted inputs[L]))
			//sum_nabla_b[num_layers] += errors[num_layers]
			//sum_nabla_w[num_layers] += (errors[num_layers]) * (activations[num_layers-1] transposed)
			//Looping an index i from num_layers-1 to 1 (the remaining layers)
				//errors[i] = hadamardProduct(((weights[i + 1] transposed)*(errors[i + 1])), sigmoidPrime(weighted inputs[i])
				//sum_nabla_b[i] += errors[i]
				//sum_nabla_w[i] += (errors[i]) * (activations[i-1] transposed)
			//end of the loop
		}
		
	//SGD's function is to complete a forward pass and backward pass on a mini batch and compute the average nabla_b and 
	//nabla_w vectors over the whole batch. it then updates the weights and biases using the nabla vectors and the learning_rate
	//SGD requires an int batch size to iterate over the vector mini_batch_indices which hold indices of the test data
	//  with respect to the mini batch to be iterated
	//  it is assumed that these indices are randomly generated and won't repeat a test element for the same epoch but that
	//  is outside SGD's scope and it won't need to worry about that.
	//SGD also requires test_data_indices to have been resized appropriately
	
	//may not be the same as mini batch size due to leftover data at end of test data
	//	We're throwing out the leftovers as per discussion in the meeting on 9-22-17 (-Eli)
	void SGD(int input_batch_size)
	{
		//Looping index i from 0 to input_batch_size
			//forwardPropagation(mini_batch_indices[i])
			//backwardPropagation(mini_batch_indices[i])
		//update();                
		//zero out sum_nabla vectors
	}
		
	void update()
	{
		//Looping index i from 1 to num_layers
			//weights[i] -= (learning_rate / batch_size) * (sum_nabla_w[i]) <-- I don't think there's a way to get around doing one division
			//biases[i]  -= (learning_rate / batch_size) * (sum_nabla_b[i])		per element. You either do it in the loop or not.
	}
		
	//Train brings all the other methods together and trains the network. It loops through the epochs separating out 
	//mini batches from the test data and running sgd on all mini batches for each epoch. Train doesnt actually access the files
	void train()
	{
		//jump to the beginning of training file using training_data_infile
		//int test_data_size = size of training data
		//test_data_indices resized to test_data_size

		//Loop index i through size of test_data_indices
			//test_data_indices[i] = i

		//int sgd_calls = (test_data_size)/(batch_size)
		//Looping index i from 0 to epochs	<-- start of epoch
			//num_correct = 0;
			//shuffle(test_data_indices)
			//Looping index j from 0 to sgd_calls
				//Looping index k from 0 to (batch_size - 1)
					//mini_batch_indices[k] = test_data_indices[(j*batch_size) + k)]
				//SGD(batch_size)
			
			//cout << "Efficiency at epoch: " << i << " = " << ((num_correct) / (test data size))*100 <<" %";
		//end of the loop
		
		//bool result = Create_Network_File(); 		//this will store the values required for classification
		//if(!result)
			//Display an error message
	}
	
	bool createNetworkFile()
	{
		//create a file named Previous_Network_[Date and Time] to write the required values 
		if (is_open(Previous_Network_[Date and Time])) 
			//write the values of hyperparameters, num_layers, array of layer sizes, matrices of weights and biases
			//close the file
			return true;
		else
			return false;
	}
		
	void classify() 				//we don't need any parameters here, because constructor stores the name of validation file in the training_data_file
	{
		//int ambiguous_data (0);
		//int numTrainingInputs;		//will tell us the number of the training inputs in the verfification file
		//int biggest;
		//to calculate numTrainingInputs, we can call tellg(), which will give us the size of file in bytes so that 
		//numTrainingInputs = size of file/size of a single verification datum
		//Loop index i from 0 to size of (verification data) //from i = 0 to end
			//call forwardPass(i), which takes a datum via training_data_filename and computes the activations <-- is this the right file?
			//set biggest = 0, which will act as the index of the biggest activation value
			//Looping index j from 1 to size of (activations[num_layers]) (j=0 is assumed to be biggest)
				//if activations(num_layers, j) > activations(num_layers, biggest) 
					//then biggest = j
				//else if activations(num_layers, j) == activations(num_layers, biggest)
					//increment ambiguous_data;
				//else
					//do nothing.
			//print out classification of biggest
		
		//print out the number of data classified, which will be (size of verification data) - ambiguous_data
			// i.e. what output the network has decided, for our purposes a string  "horizontal" "vertical" or "diagonal"
	}
		
	//hadamardProduct() is a operation between two matrices where we obtain a matrix by multiplying an element in one matrix 
    //with that respective element in the other matrix.
	//hadamardProduct requires that M1 and M1 are of equal size
   	matrix& hadamardProduct (const matrix& M1, const matrix& M2)
	{
	  //matrix M;
	  //check the size of the matrices
	  //if equal:
		  //Create a matrix M of same size as that of either M1 or M2
		  //Start the loop for i = 1 to i = the number of columns in M1 
		      //Start an inner loop for j = 1 to j = number of rows in M1
			  	//Every element in M, M(j, i) = M1(j, i) * M2(j, i); <-- how dlib handles single elements, iirc
		      //end of the loop
		  //end of the loop
		  //return the matrix M

	      //else
		  //throw an exception or display the error message, return an empty matrix;
	}
	  
	//shuffleDataIndices() will be called at the starting of every epoch to randomize the data provided by the user
	//This will create a vector<int> of the same size as that of Data Set and every element of the vector will point to 
	//a particular input data in the data file.
	//shuffleDataIndices() will randomize this array of int by swapping different indices, using Fisher-Yates shuffle
	//This will assign created vector<int>, once done with the shuffling
	void shuffleDataIndices(vector<T> data_indices)
	{
	  //Loop index i from 0 to (size of expected_values data set - mini_batch - 1)
		//j = random integer in the range [mini_batch + i, size of expected_values data set]
		//exchange data_indices[i] with data_indices[j];
	}
};

//Functions outside of the class will include:
matrix& activationFunction(const matrix& weighted_inputs, const int Layer_Size)
{
	//return sigmoid(weighted_inputs)
}

matrix& costPrime(const matrix& input_activation_layer, const matrix& expected_val)
{
	//creating an entire matrix every time you need to evaluate cost_prime will extend the time to process data
	
	//return (input_activation_layer - expected_val)
}

matrix& activationPrime(const matrix& input_matrix)
{
	//we don't need to do this element-by-element because dlib's sigmoid function can handle an
	//entire matrix as input, we need the Hadamard product though
	
	//Make a matrix of ones, named ones, size the same as input_matrix
	//return hadamardProduct(sigmoid(input_matrix), ones - sigmoid(input_matrix))
}

//doubtful about including Randomize_Weights() outside the class
//	The user may want to mess with the distribution, 
//	but we can write it into the class for now if it helps troubleshooting (-Eli)
