
//Class Network
   //this will extract the data from the data file as input layer
   //vector in this context refers to the c++ data structure
   //vectors can be swapped for c-style arrays wherevever it's more efficient
   
     //Parameters:     
         //vector of ints (layers), number of neurons per layer 
         //    (not necessary to be declared as a class data memeber since it's a one use parameter)
            //number of layers L = layers.size()
            //layers are 0 indexed so first layer is 0, last layer is L-1
         //vector of weight matrices (weights) where:
            // weights.size() = L-1
            // weights[i] = a matrix of size (layers[i+1] x layers[i])
            // weights[i] = weights in between layers i, i+1 (weights[2] is w3)
         //vector of bias matrices (biases) where:
            // biases.size() = L-1
            // biases[i] = matrix of size (layers[i+1] x 1)
            // biases[i] = biases of (i+1)th layer (biases[2] is b3)
     //hyperparameters:
         //double, learning rate (eta)
         //int, batch size (n)
         //int, number of epochs (E)
    
         
^^^number of neurons per layer can be discarded after setup, so we may not need it as an attribute, just a method variable^^^           
     
   //Constructors
      //one that accepts an int array, a doubles (eta), 2 ints (batch size, epochs)
         //initializes weights, biases based on some normal
      //default one with default values to be decided at a later time
      //destructor
   
   //Methods:
        //Read-in() which will read all the required parameters from the keyboard (neurons per layer, hyperparameters)
        //  Read-in should probably be defined by main right before declaring a Network object
        //  data from read-in translated into parameters for Network constructor
        
        //  The reason for this is that when the object is declared in main, it already needs to have its data memebers initialized
        //  and it wouldn't make sense to call a method of the class after it is declared with default values
        
        //  another option is to make read-in a static methiod so it can still retain class membership (probably cleaner solution)
         
            //this will also [be overloaded to?] extract the required 4-pixel image from the data file
        //CreateVectors() with no. of hidden layers and no. of neurons in each layer as parameters
            //this function will create vectors for the hidden layers
            //and, this function will create weight matrices as well, 
            //then populate the weights with random numbers, normal distribution, mean 0, standard deviation 1/Sqrt[m(l-1)]
 
 //Inherited class from Network: Class Forward Pass
 // should be a method not a new class
    //this will assign each with the activation values and will get us the outputs for the whole network
    
      //Parameters:
      //Input file for testing data
      //weighted input vectors [z2, z3, ..., zl, ..., zL]
         //sizes will match [m2, m3, ..., ml, ..., mL]
      //activation vectors [a1, a2, ..., al, ..., aL]
         //sizes will match [m2, m3, ..., ml, ..., mL]
      //weighted inputs for each layer
      //activation values for each layer
      //vector for output neurons
      
      //Methods:
        //Calculate_Weighted_Input() inline function which will have method to calculate the weighted input
        //Calculate_Activation_Value() inline function which will have method to calculate the Activation value
            //will necessarily include sigmoid activation function
        //Consider including the latter methods in the two functions above, since our outputs will all be matrices, we can just
            //assign the [zl] and [al] during calculation
               //Assign_Weighted_inputs()
               //Assign_Activation_Values()
      
//Inherited class from Forward Pass: Class Backpropagation
   //this will calculate the errors and change the weights and biases
   //this will also store all the values of the cost gradients wrt biases and wrt weights
   
      //Parameters/Attributes:
      //Expected values from the training data file
      //nabla_b
      //nabla_w
            
      //Methods/Behaviors:
      //Hadamarde Product (the one in dlib doesn't quite do what I expected)
      //Vectorized Cost Function
      //First derivative of Vectorized Cost Function (wrt activations)
      //Calculate Error vectors (~ nabla_b) for output layer
      //Calculate Error vectors (~ nabla_b) for inner nodes 
      //Assign values to nabla_w vectors
      //Vectorized Activation Function
      //First derivative of Activation Function (wrt weighted inputs)

//Inhereted class from Forward Pass and Backprop: Stochastic Gradient Descent
     //SGD requires a forward pass and a backward pass per sample in each mini batch
     //Will execute FP then BP, keeping an average of cost partials, then update the weights and biases
     //until all samples in the training data are exhausted.
     //This comprises one epoch, so do this entire process as many times as the user requests
      
         //Parameters/Attributes:
         //(none) (Could be the learning rate, epochs and the batch size (**if the forward and backprop are not publicly inherited))
      
         //Methods/Behaviors:
         //Stochatic Gradient Descent function
            //chooses n random samples from training data, feeds forward, props back once per n
            //keeps total nabla_w and nabla_b
         //Update function for weights
            //w += (eta/n)nabla_w
         //Update function for biases
            //b += (eta/n)nabla_b
         //Progress output to console
            //after every mini-batch, display (# of samples within Threshold)/(total samples tested)

                           ^^^ May be able to combine this with Backpropagation class ^^^
         
                                       ~~~STUFF WE THREW AROUND IN PREVIOUS MEETINGS~~~  
Save files, load files for network config
threshold for classification
   checks which Cost is lowest out of possible output categories
   if Cost is below a number k, that classifier is chosen
   otherwise, discard as ambiguous
